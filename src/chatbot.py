import os
import chromadb
import gradio as gr
from openai import OpenAI
from dotenv import load_dotenv
from typing import List, Dict, Set
import time
import re
from collections import Counter
import math

# Load environment variables and initialize clients
load_dotenv()
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
chroma_client = chromadb.PersistentClient(path="./chroma_db")
embedding_function = chromadb.utils.embedding_functions.OpenAIEmbeddingFunction(
    api_key=os.getenv('OPENAI_API_KEY'),
    model_name="text-embedding-3-small"
)
collection = chroma_client.get_collection(name="scholarship-qa", embedding_function=embedding_function)

# Simplified response cache with size limit
_response_cache = {}
MAX_CACHE_SIZE = 30

# Category keyword mapping with inverse index for fast lookup
CATEGORY_KEYWORDS = {
    "Scholarship": ["ƒëi·ªÅu ki·ªán", "y√™u c·∫ßu", "ti√™u chu·∫©n", "ti√™u ch√≠", "ƒë·ªß ƒëi·ªÅu ki·ªán", "ƒë√°p ·ª©ng", "h·ªçc b·ªïng", "khuy·∫øn kh√≠ch"],
    "Decree": ["quy tr√¨nh", "th·ªß t·ª•c", "c√°c b∆∞·ªõc", "qu√° tr√¨nh", "th·ª±c hi·ªán", "ngh·ªã ƒë·ªãnh", "n·ªôp h·ªì s∆°"],
    "Timeline": ["th·ªùi gian", "th·ªùi h·∫°n", "khi n√†o", "h·∫°n cu·ªëi", "deadline", "l·ªãch tr√¨nh"]
}
KEYWORD_TO_CATEGORY = {kw: cat for cat, keywords in CATEGORY_KEYWORDS.items() for kw in keywords}

def detect_categories(query: str) -> Set[str]:
    """Detect categories from query using keyword matching with improved fallback"""
    query_lower = query.lower()
    detected = {KEYWORD_TO_CATEGORY[kw] for kw in KEYWORD_TO_CATEGORY if kw in query_lower}
    
    # More flexible detection - if no categories detected, we'll use pure vector search
    return detected

def build_category_filter(categories: Set[str]):
    """Build ChromaDB filter from categories"""
    if not categories:
        return None
        
    if len(categories) == 1:
        category = next(iter(categories))
        return {"$or": [{"category": category}, {"subcategory": category}]}
    
    return {"$or": [{"category": cat} for cat in categories] + [{"subcategory": cat} for cat in categories]}

def get_relevant_content(query: str, use_categories: bool = False, final_results: int = 4) -> List[Dict]:
    """Get relevant content with hybrid retrieval and reranking"""
    start = time.time()
    
    # Build filter based on detected categories
    filter_dict = None
    categories = set()
    
    if use_categories:
        categories = detect_categories(query)
        if categories:
            print(f"Detected categories: {categories}")
            filter_dict = build_category_filter(categories)
    
    try:
        # First stage: retrieve initial results
        initial_results = 10
        
        # If no categories detected, use pure vector search with more results
        if not categories and use_categories:
            print("No categories detected, using pure vector similarity search")
        
        results = collection.query(
            query_texts=[query],
            n_results=initial_results,
            where=filter_dict,
            include=["metadatas", "documents"]
        )
        
        # Process results
        candidate_content = []
        if results['metadatas'] and results['metadatas'][0]:
            for i, metadata in enumerate(results['metadatas'][0]):
                document_text = results['documents'][0][i] if results['documents'] and results['documents'][0] else ''
                candidate_content.append({
                    'question': metadata.get('question', ''),
                    'answer': metadata.get('answer', ''),
                    'category': metadata.get('category', ''),
                    'subcategory': metadata.get('subcategory', ''),
                    'source': metadata.get('source', ''),
                    'document': document_text,
                    'relevance_score': 0.0
                })
        
        # Apply BM25 reranking if we have candidates
        if candidate_content:
            print(f"Reranking {len(candidate_content)} initial results...")
            
            # Extract documents for BM25
            documents = [f"{item['question']} {item['answer']}" for item in candidate_content]
            
            # Initialize BM25 with the retrieved documents
            bm25 = BM25(documents)
            
            # Get BM25 scores
            bm25_scores = bm25.get_scores(query)
            
            # Add BM25 scores and sort
            for i, score in enumerate(bm25_scores):
                candidate_content[i]['relevance_score'] = score
            
            # Rerank based on BM25 scores
            candidate_content.sort(key=lambda x: x.get('relevance_score', 0), reverse=True)
            
            # Limit to final results
            final_content = candidate_content[:final_results]
            
            print(f"Retrieval time: {time.time() - start:.4f}s, Final results: {len(final_content)}")
            return final_content
        else:
            print("No initial results found.")
            return []
    except Exception as e:
        print(f"Error during retrieval: {str(e)}")
        return []

def extract_source_info(text: str) -> str:
    """Extract document references from text with simplified pattern"""
    pattern = r'(?:Ngh·ªã ƒë·ªãnh|Quy·∫øt ƒë·ªãnh|Th√¥ng t∆∞|VƒÉn b·∫£n|Nƒê|Qƒê|TT|CV)(?:\s*s·ªë\s*)?([\d\/\-]+[\w\-]*)'
    matches = set(re.findall(pattern, text, re.IGNORECASE))
    return ", ".join(f"VƒÉn b·∫£n {m}" for m in matches) if matches else ""

def create_prompt(query: str, content: List[Dict]) -> str:
    """Create optimized prompt from retrieved content"""
    context_items = []
    
    for item in content:
        if not item.get('answer') or not item['answer'].strip():
            continue
        
        context_str = f"Q: {item.get('question', '')}\nA: {item.get('answer', '')}"
        source_info = extract_source_info(item.get('answer', ''))

        if not source_info and item.get('source'):
            source_info = item.get('source', '').split('/')[-1]
            
        if source_info:
            context_str += f"\nNgu·ªìn: {source_info}"
        
        context_items.append(context_str)
    
    if not context_items:
        context_items = ["Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p."]
    
    combined_context = "\n\n---\n\n".join(context_items)
    prompt = """Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin sau:
    
    {0}
    
    C√ÇU H·ªéI: {1}
    
    H∆∞·ªõng d·∫´n:
    1. B·ªè qua nh·ªØng th√¥ng tin kh√¥ng li√™n quan ƒë·∫øn c√¢u h·ªèi.
    2. T·ªïng h·ª£p c√°c ƒëi·ªÉm chung gi·ªØa c√°c ngu·ªìn th√¥ng tin ƒë·ªÉ ƒë∆∞a ra c√¢u tr·∫£ l·ªùi t·ªïng h·ª£p, ƒë√°p ·ª©ng v·ªõi y√™u c·∫ßu c·ªßa c√¢u h·ªèi.
    3. N·∫øu c√≥ th√¥ng tin m√¢u thu·∫´n, ∆∞u ti√™n th√¥ng tin t·ª´ ngu·ªìn m·ªõi nh·∫•t.
    4. Tr·∫£ l·ªùi m·ªôt c√°ch r√µ r√†ng v√† c√≥ c·∫•u tr√∫c.
    5. Khi li·ªát k√™ c√°c ƒëi·ªÅu ki·ªán ho·∫∑c b∆∞·ªõc, tr√¨nh b√†y d∆∞·ªõi d·∫°ng danh s√°ch c√≥ ƒë√°nh s·ªë.
    6. Tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu c·ª• th·ªÉ ·ªü cu·ªëi c√¢u tr·∫£ l·ªùi.
    
    TR·∫¢ L·ªúI:""".format(combined_context, query)
    
    return prompt

def get_llm_response(prompt: str) -> str:
    """Get response from language model"""
    start = time.time()
    
    response = client.chat.completions.create(
        model="gpt-4o-mini",
        messages=[
            {"role": "system", "content": 
             "B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n th√¥ng tin chuy√™n nghi·ªáp. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß. "
             "S·ª≠ d·ª•ng ƒë·ªãnh d·∫°ng r√µ r√†ng v·ªõi c√°c ƒëi·ªÉm ch√≠nh ƒë∆∞·ª£c tr√¨nh b√†y d∆∞·ªõi d·∫°ng danh s√°ch. "
             "Tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu r√µ r√†ng. Th·ª´a nh·∫≠n khi kh√¥ng c√≥ ƒë·ªß th√¥ng tin."
            },
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=600,
        presence_penalty=0.3
    )
    
    print(f"LLM response time: {time.time() - start:.4f}s")
    return response.choices[0].message.content

def process_user_query(query: str) -> str:
    try:
        start = time.time()
        
        # Simple cache check (exact match only)
        normalized_query = ' '.join(query.lower().split())
        if normalized_query in _response_cache:
            print(f"Cache hit for query: '{normalized_query}'")
            print(f"Response time: {time.time() - start:.2f}s")
            return _response_cache[normalized_query]
        
        # Adjust number of results based on query complexity
        n_results = 4 if len(query.split()) > 6 else 5
        
        # First try with category filtering
        relevant_content = get_relevant_content(query, use_categories=False, final_results=n_results)
        
        # If no results or very few results with categories, fall back to pure vector search
        if len(relevant_content) < 2:
            print("Insufficient results with categories, falling back to pure vector search")
            relevant_content = get_relevant_content(query, use_categories=False, final_results=5)
        
        # Generate response
        prompt = create_prompt(query, relevant_content)
        response = get_llm_response(prompt)
        
        # Update cache (limit size)
        if len(_response_cache) >= MAX_CACHE_SIZE:
            _response_cache.pop(next(iter(_response_cache)))
        _response_cache[normalized_query] = response
        
        print(f"Total response time: {time.time() - start:.2f}s\n")
        return response
        
    except Exception as e:
        print(f"Error processing query: {str(e)}")
        return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"

class BM25:
    """BM25 scoring algorithm for reranking search results"""
    def __init__(self, documents, k1=1.5, b=0.75):
        self.k1 = k1
        self.b = b
        self.tokenized_docs = [self.tokenize(doc) for doc in documents]
        self.doc_lengths = [len(doc) for doc in self.tokenized_docs]
        self.avgdl = sum(self.doc_lengths) / len(self.doc_lengths) if self.doc_lengths else 0
        self.term_frequencies = [Counter(doc) for doc in self.tokenized_docs]
        self.doc_freqs = Counter()
        for doc in self.tokenized_docs:
            for term in set(doc):
                self.doc_freqs[term] += 1
        self.idfs = {term: self._idf(term) for term in self.doc_freqs}
    
    def tokenize(self, text):
        """Simple tokenization function"""
        text = re.sub(r'[^\w\s]', '', text.lower())
        return text.split()
    
    def _idf(self, term):
        """Calculate IDF for a term"""
        return math.log((len(self.tokenized_docs) - self.doc_freqs[term] + 0.5) / 
                        (self.doc_freqs[term] + 0.5) + 1.0)
    
    def get_scores(self, query):
        """Calculate BM25 scores for a query across all documents"""
        query_terms = self.tokenize(query)
        scores = [0.0] * len(self.tokenized_docs)
        
        for term in query_terms:
            if term not in self.idfs:
                continue
            for i, doc in enumerate(self.tokenized_docs):
                if term not in self.term_frequencies[i]:
                    continue
                freq = self.term_frequencies[i][term]
                numerator = self.idfs[term] * freq * (self.k1 + 1)
                denominator = freq + self.k1 * (1 - self.b + self.b * self.doc_lengths[i] / self.avgdl)
                scores[i] += numerator / denominator
        return scores

# Gradio interface
def chatbot_interface(message, history):
    return process_user_query(message)

# Configure Gradio
demo = gr.ChatInterface(
    fn=chatbot_interface,
    title="Tr·ª£ l√Ω t∆∞ v·∫•n H·ªçc b·ªïng üéì",
    description="H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ h·ªçc b·ªïng, m√¨nh s·∫Ω tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† nhanh ch√≥ng!",
    theme=gr.themes.Soft(),
    examples=[
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ nh·∫≠n h·ªçc b·ªïng l√† g√¨?",
        "Quy tr√¨nh x√©t h·ªçc b·ªïng nh∆∞ th·∫ø n√†o?",
        "Sinh vi√™n t·∫°i Tr∆∞·ªùng ƒê·∫°i h·ªçc C√¥ng ngh·ªá Th√¥ng tin v√† Truy·ªÅn th√¥ng c√≥ th·ªÉ ph·∫£n √°nh, th·∫Øc m·∫Øc v·ªÅ danh s√°ch h·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p trong th·ªùi gian bao l√¢u v√† theo c√°ch n√†o?",
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c x√©t h·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p?"
    ]
)

if __name__ == "__main__":
    demo.queue(max_size=10)
    demo.launch(
        server_name="127.0.0.1", 
        server_port=7860, 
        share=True,
        show_error=True
    )