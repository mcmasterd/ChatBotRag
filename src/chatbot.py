import os
import chromadb
import gradio as gr
from openai import OpenAI
from dotenv import load_dotenv
import time
import re
from typing import List, Dict, Set, Optional

# Initialize clients
load_dotenv()
client = OpenAI(api_key=os.getenv('OPENAI_API_KEY'))
chroma_client = chromadb.PersistentClient(path="./chroma_db")
collection = chroma_client.get_collection(
    name="scholarship-qa",
    embedding_function=chromadb.utils.embedding_functions.OpenAIEmbeddingFunction(
        api_key=os.getenv('OPENAI_API_KEY'),
        model_name="text-embedding-3-small"
    )
)

# Simple cache and category mapping
_response_cache = {}
CATEGORIES = {
    "Scholarship": ["ƒëi·ªÅu ki·ªán", "y√™u c·∫ßu", "ti√™u chu·∫©n", "h·ªçc b·ªïng", "khuy·∫øn kh√≠ch"],
    "Decree": ["quy tr√¨nh", "th·ªß t·ª•c", "c√°c b∆∞·ªõc", "ngh·ªã ƒë·ªãnh", "n·ªôp h·ªì s∆°"],
    "Timeline": ["th·ªùi gian", "th·ªùi h·∫°n", "khi n√†o", "deadline", "l·ªãch tr√¨nh"]
}
KEYWORD_TO_CATEGORY = {kw: cat for cat, keywords in CATEGORIES.items() for kw in keywords}

# Document source extraction patterns
SOURCE_PATTERNS = {
    r'(?:Ngh·ªã\s*ƒë·ªãnh|Nƒê)(?:\s*s·ªë\s*)?([\d\/\-]+[\w\-]*)': 'Ngh·ªã ƒë·ªãnh',
    r'(?:Quy·∫øt\s*ƒë·ªãnh|Qƒê)(?:\s*s·ªë\s*)?([\d\/\-]+[\w\-]*)': 'Quy·∫øt ƒë·ªãnh',
    r'(?:Th√¥ng\s*t∆∞|TT)(?:\s*s·ªë\s*)?([\d\/\-]+[\w\-]*)': 'Th√¥ng t∆∞',
    r'(?:VƒÉn\s*b·∫£n|C√¥ng\s*vƒÉn|CV)(?:\s*s·ªë\s*)?([\d\/\-]+[\w\-]*)': 'VƒÉn b·∫£n'
}

def extract_sources(text: str) -> str:
    """Extract document references from text"""
    sources = []
    for pattern, doc_type in SOURCE_PATTERNS.items():
        for match in re.finditer(pattern, text, re.IGNORECASE):
            doc_id = match.group(1).strip()
            if doc_id:
                sources.append(f"{doc_type} {doc_id}")
    
    # Add date if available
    date_match = re.search(r'ng√†y\s*(\d{1,2}[\/\-\.]\d{1,2}[\/\-\.]\d{2,4})', text, re.IGNORECASE)
    if date_match and sources:
        sources[-1] += f" ng√†y {date_match.group(1)}"
    
    return ", ".join(sources) if sources else ""

def detect_categories(query: str) -> Set[str]:
    """Identify query categories for faster retrieval"""
    query_lower = query.lower()
    
    # Direct keyword matching
    detected = {KEYWORD_TO_CATEGORY[kw] for kw in KEYWORD_TO_CATEGORY if kw in query_lower}
    
    # Add fallback category
    if not detected and any(term in query_lower for term in ["h·ªçc b·ªïng", "scholarship"]):
        detected.add("Scholarship")
        
    return detected

def get_relevant_content(query: str, use_categories: bool = True) -> List[Dict]:
    """Retrieve relevant content using hybrid search"""
    start = time.time()
    
    # Build category filter
    filter_dict = None
    if use_categories:
        categories = detect_categories(query)
        print(f"Categories: {categories}")
        
        if categories:
            if len(categories) == 1:
                cat = next(iter(categories))
                filter_dict = {"$or": [{"category": cat}, {"subcategory": cat}]}
            else:
                filter_dict = {"$or": [{"category": cat} for cat in categories] + 
                                      [{"subcategory": cat} for cat in categories]}
    
    # Query database
    try:
        results = collection.query(
            query_texts=[query],
            n_results=5 if len(query.split()) <= 3 else 3,  # More results for short queries
            where=filter_dict
        )
        
        # Process results
        content = []
        if results['metadatas'] and results['metadatas'][0]:
            for metadata in results['metadatas'][0]:
                content.append({k: metadata.get(k, '') for k in 
                               ['question', 'answer', 'category', 'subcategory']})
        
        print(f"Retrieval: {time.time() - start:.2f}s, Results: {len(content)}")
        return content
    except Exception as e:
        print(f"Retrieval error: {str(e)}")
        return []

def find_similar_query(query: str) -> Optional[str]:
    """Find cached similar query"""
    normalized = ' '.join(query.lower().split())
    
    # Exact match
    if normalized in _response_cache:
        return normalized
    
    # Substring match with significant overlap
    for cached in _response_cache:
        if ((normalized in cached and len(normalized) > 10 and len(normalized) / len(cached) > 0.7) or
            (cached in normalized and len(cached) > 10 and len(cached) / len(normalized) > 0.7)):
            return cached
            
    return None

def create_prompt(query: str, content: List[Dict]) -> str:
    """Create optimized prompt with source citations"""
    context_items = []
    
    for item in content:
        if not item.get('answer', '').strip():
            continue
            
        context = f"Q: {item.get('question', '')}\nA: {item.get('answer', '')}"
        sources = extract_sources(item.get('answer', ''))
        if sources:
            context += f"\nNgu·ªìn: {sources}"
        context_items.append(context)
    
    if not context_items:
        context_items = ["Kh√¥ng c√≥ th√¥ng tin ph√π h·ª£p."]
    
    return """Tr·∫£ l·ªùi c√¢u h·ªèi d·ª±a tr√™n th√¥ng tin sau:
    
    {0}
    
    C√ÇU H·ªéI: {1}
    
    H√£y tr·∫£ l·ªùi ng·∫Øn g·ªçn, ƒë·∫ßy ƒë·ªß d·ª±a tr√™n th√¥ng tin ƒë√£ cung c·∫•p. Tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu 
    (ngh·ªã ƒë·ªãnh, quy·∫øt ƒë·ªãnh, vƒÉn b·∫£n) n·∫øu c√≥. N·∫øu kh√¥ng ƒë·ªß th√¥ng tin, h√£y n√≥i r√µ.
    
    TR·∫¢ L·ªúI:""".format("\n\n".join(context_items), query)

def get_response(prompt: str) -> str:
    """Get optimized LLM response"""
    start = time.time()
    
    response = client.chat.completions.create(
        model="gpt-3.5-turbo",
        messages=[
            {"role": "system", "content": 
             "B·∫°n l√† tr·ª£ l√Ω t∆∞ v·∫•n h·ªçc b·ªïng chuy√™n nghi·ªáp. Tr·∫£ l·ªùi ng·∫Øn g·ªçn, ch√≠nh x√°c v√† ƒë·∫ßy ƒë·ªß. "
             "Tr√≠ch d·∫´n ngu·ªìn t√†i li·ªáu r√µ r√†ng khi c√≥ th√¥ng tin. T·ª± tin v·ªõi th√¥ng tin ƒë√£ c√≥."
            },
            {"role": "user", "content": prompt}
        ],
        temperature=0.3,
        max_tokens=400,
        presence_penalty=0.3
    )
    
    print(f"LLM time: {time.time() - start:.2f}s")
    return response.choices[0].message.content

def process_query(query: str) -> str:
    """Main query processing pipeline"""
    try:
        start = time.time()
        
        # Check cache
        similar = find_similar_query(query)
        if similar:
            print(f"Cache hit: '{similar}'")
            return _response_cache[similar]
        
        # Retrieve content with categories, fallback if needed
        content = get_relevant_content(query, use_categories=True)
        if not content:
            content = get_relevant_content(query, use_categories=False)
        
        # Generate response
        prompt = create_prompt(query, content)
        response = get_response(prompt)
        
        # Update cache
        _response_cache[' '.join(query.lower().split())] = response
        
        print(f"Total time: {time.time() - start:.2f}s")
        return response
        
    except Exception as e:
        import traceback
        print(f"Error: {str(e)}")
        print(traceback.format_exc())
        return f"Xin l·ªói, ƒë√£ c√≥ l·ªói x·∫£y ra: {str(e)}"

# Gradio interface
demo = gr.ChatInterface(
    fn=lambda message, history: process_query(message),
    title="Tr·ª£ l√Ω t∆∞ v·∫•n H·ªçc b·ªïng üéì",
    description="H√£y ƒë·∫∑t c√¢u h·ªèi v·ªÅ h·ªçc b·ªïng, m√¨nh s·∫Ω tr·∫£ l·ªùi ƒë·∫ßy ƒë·ªß v√† nhanh ch√≥ng!",
    theme=gr.themes.Soft(),
    examples=[
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ nh·∫≠n h·ªçc b·ªïng l√† g√¨?",
        "Quy tr√¨nh x√©t h·ªçc b·ªïng nh∆∞ th·∫ø n√†o?",
        "Th·ªùi gian x√©t h·ªçc b·ªïng l√† khi n√†o?",
        "ƒêi·ªÅu ki·ªán ƒë·ªÉ ƒë∆∞·ª£c x√©t h·ªçc b·ªïng khuy·∫øn kh√≠ch h·ªçc t·∫≠p?"
    ]
)

if __name__ == "__main__":
    demo.queue(max_size=10)
    demo.launch(server_name="127.0.0.1", server_port=7860, share=True)